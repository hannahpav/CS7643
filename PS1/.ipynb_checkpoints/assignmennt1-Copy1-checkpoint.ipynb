{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3c974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import main\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6c12f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data loaded with 60000 images\n",
      "Loading testing data...\n",
      "Testing data loaded with 10000 images\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, val_data, val_label = utils.load_mnist_trainval()\n",
    "test_data, test_label = utils.load_mnist_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ae738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ca4ef2",
   "metadata": {},
   "source": [
    "A - input layer\n",
    "Z - not active middle later\n",
    "b - add bias term\n",
    "then apply an activation function\n",
    "activation function by tanh or sigmoid, now its not linear\n",
    "    can be more complex or powreful\n",
    "using relu, which is like an elbow\n",
    "Z2, unactivated secon layer is second weight parameter, between first and second layteers, times activiated irst layer, plus bias\n",
    "    now we apply softmax activation function\n",
    "e to a single node is  apart of a whole\n",
    "\n",
    "forward prop, put in stuff an predict image\n",
    "\n",
    "back prop\n",
    "how much did prediction deviate rom the actual label and adjust accordingly\n",
    "dz2 is loss of second later\n",
    "db2 as average of absolute error\n",
    "taking error from second layer and applying weigts in reverse to first\n",
    "need to undo activation to get error from firt layer\n",
    "\n",
    "then update parameters accordingly\n",
    "alpha as a hyper-parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8950ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nptrain = np.array(train_data)\n",
    "nptrain = nptrain.T\n",
    "nptrain_label = np.array(train_label)\n",
    "\n",
    "nptest = np.array(test_data).T\n",
    "nptest_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638be70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = nptrain.shape\n",
    "nptrain = nptrain/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "971194cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 48000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nptrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e330e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "# subtract .5 so we have values between  0.5 - 0.5\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.randn(10,n) - 0.5\n",
    "    b1 = np.random.rand(10,1) - 0.5\n",
    "    W2 = np.random.randn(10,10) - 0.5\n",
    "    b2 = np.random.rand(10,10) - 0.5 \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "    \n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0\n",
    "    \n",
    "def softmax(Z):\n",
    "    return (np.exp(Z) / sum(np.exp(Z)))\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    print(A1.shape)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "    \n",
    "def one_hot(Y):\n",
    "    # assumes that are classes 0-9, and we want 10 classes, so 9+1\n",
    "    # index through one_hot_y to make a range, and Y is all the labels\n",
    "    # for each row, go to the column specified by the label in Y and set it to 1\n",
    "    one_hot_Y = np.zeroes((Y.size, Y.max()+1))\n",
    "    one_hot_Y[np.arnage(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "    \n",
    "def back_prop(Z1, A1, Z2, A2, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot(Y)\n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2)  * deriv_ReLU(Z1)\n",
    "    \n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dZ1)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a7cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    print(W2.shape)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Iteraction: \", i)\n",
    "            print(\"Accuracy: \", get_accuracy(get_predications(A2), Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f784b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 48000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,48000) (10,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnptrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnptrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(W2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m----> 5\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m back_prop(Z1, A1, Z2, A2, W2, X, Y)\n\u001b[0;32m      7\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36mforward_prop\u001b[1;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[0;32m     22\u001b[0m A1 \u001b[38;5;241m=\u001b[39m ReLU(Z1)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(A1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 24\u001b[0m Z2 \u001b[38;5;241m=\u001b[39m \u001b[43mW2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\n\u001b[0;32m     25\u001b[0m A2 \u001b[38;5;241m=\u001b[39m softmax(Z2)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Z1, A1, Z2, A2\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,48000) (10,10) "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(nptrain, nptrain_label, .1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, m_train = nptrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ed6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cf389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "o, p = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:p]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:o].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:p]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d48321",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8206290",
   "metadata": {},
   "outputs": [],
   "source": [
    "nptrain.shape, nptrain_label.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
