{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3c974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import main\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, train_label, val_data, val_label = utils.load_mnist_trainval()\n",
    "# test_data, test_label = utils.load_mnist_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fe988",
   "metadata": {},
   "source": [
    "A - input layer\n",
    "Z - not active middle later\n",
    "b - add bias term\n",
    "then apply an activation function\n",
    "activation function by tanh or sigmoid, now its not linear\n",
    "    can be more complex or powreful\n",
    "using relu, which is like an elbow\n",
    "Z2, unactivated secon layer is second weight parameter, between first and second layteers, times activiated irst layer, plus bias\n",
    "    now we apply softmax activation function\n",
    "e to a single node is  apart of a whole\n",
    "\n",
    "forward prop, put in stuff an predict image\n",
    "\n",
    "back prop\n",
    "how much did prediction deviate rom the actual label and adjust accordingly\n",
    "dz2 is loss of second later\n",
    "db2 as average of absolute error\n",
    "taking error from second layer and applying weigts in reverse to first\n",
    "need to undo activation to get error from firt layer\n",
    "\n",
    "then update parameters accordingly\n",
    "alpha as a hyper-parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf266533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nptrain = np.array(train_data)\n",
    "# nptrain = nptrain.T\n",
    "# nptrain_label = np.array(train_label)\n",
    "\n",
    "# nptest = np.array(test_data).T\n",
    "# nptest_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, m = nptrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_params():\n",
    "#     W1 = np.random.rand(10, 784) - 0.5\n",
    "#     b1 = np.random.rand(10, 1) - 0.5\n",
    "#     W2 = np.random.rand(10, 10) - 0.5\n",
    "#     b2 = np.random.rand(10, 1) - 0.5\n",
    "#     return W1, b1, W2, b2\n",
    "\n",
    "# def ReLU(Z):\n",
    "#     return np.maximum(Z, 0)\n",
    "\n",
    "# def softmax(Z):\n",
    "#     A = np.exp(Z) / sum(np.exp(Z))\n",
    "#     return A\n",
    "\n",
    "# def forward_prop(W1, b1, W2, b2, X):\n",
    "#     Z1 = W1.dot(X) + b1\n",
    "#     A1 = ReLU(Z1)\n",
    "#     Z2 = W2.dot(A1) + b2\n",
    "#     A2 = softmax(Z2)\n",
    "#     return Z1, A1, Z2, A2\n",
    "\n",
    "# def deriv_ReLU(Z):\n",
    "#     return Z > 0\n",
    "\n",
    "# def one_hot(Y):\n",
    "#     one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "#     one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "#     one_hot_Y = one_hot_Y.T\n",
    "#     return one_hot_Y\n",
    "\n",
    "# def back_prop(Z1, A1, Z2, A2, W2, X, Y):\n",
    "#     one_hot_Y = one_hot(Y)\n",
    "#     dZ2 = A2 - one_hot(Y)\n",
    "#     dW2 = 1/m * dZ2.dot(A1.T)\n",
    "#     db2 = 1/m * np.sum(dZ2)\n",
    "#     dZ1 = W2.T.dot(dZ2)  * deriv_ReLU(Z1)\n",
    "    \n",
    "#     dW1 = 1/m * dZ1.dot(X.T)\n",
    "#     db1 = 1/m * np.sum(dZ1)\n",
    "    \n",
    "#     return dW1, db1, dW2, db2\n",
    "\n",
    "# def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "#     W1 -= alpha * dW1\n",
    "#     b1 -= alpha * db1\n",
    "#     W2 -= alpha * dW2\n",
    "#     b2 -= alpha * db2\n",
    "#     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207611e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37afe28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd2ae9b",
   "metadata": {},
   "source": [
    "# initialize parameters\n",
    "# subtract .5 so we have values between  0.5 - 0.5\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.randn(10,n) - 0.5\n",
    "    b1 = np.random.rand(10,1) - 0.5\n",
    "    W2 = np.random.randn(10,10) - 0.5\n",
    "    b2 = np.random.rand(10,10) - 0.5 \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "    \n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0\n",
    "    \n",
    "def softmax(Z):\n",
    "    return (np.exp(Z) / sum(np.exp(Z)))\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    print(W2.shape, A1.shape)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "    \n",
    "def one_hot(Y):\n",
    "    # assumes that are classes 0-9, and we want 10 classes, so 9+1\n",
    "    # index through one_hot_y to make a range, and Y is all the labels\n",
    "    # for each row, go to the column specified by the label in Y and set it to 1\n",
    "    one_hot_Y = np.zeroes((Y.size, Y.max()+1))\n",
    "    one_hot_Y[np.arnage(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "    \n",
    "def back_prop(Z1, A1, Z2, A2, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot(Y)\n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2)  * deriv_ReLU(Z1)\n",
    "    \n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dZ1)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec26e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predictions(A2):\n",
    "#     return np.argmax(A2, 0)\n",
    "\n",
    "# def get_accuracy(predictions, Y):\n",
    "#     print(predictions, Y)\n",
    "#     return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "# def gradient_descent(X, Y, alpha, iterations):\n",
    "#     W1, b1, W2, b2 = init_params()\n",
    "#     for i in range(iterations):\n",
    "#         Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "#         dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, Y)\n",
    "#         W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "#         if (i % 10 == 0):\n",
    "#             print(\"Iteraction: \", i)\n",
    "#             print(\"Accuracy: \", get_accuracy(get_predictions(A2), Y))\n",
    "#     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cdbcda",
   "metadata": {},
   "source": [
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Iteraction: \", i)\n",
    "            print(\"Accuracy: \", get_accuracy(get_predications(A2), Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b794cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1, b1, W2, b2 = gradient_descent(nptrain, nptrain_label, .1, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac262bb5",
   "metadata": {},
   "source": [
    "## SOFTMAX TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6758fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data loaded with 60000 images\n",
      "Loading testing data...\n",
      "Testing data loaded with 10000 images\n",
      "i=0\n",
      "i=32\n",
      "i=64\n",
      "i=96\n",
      "i=128\n",
      "i=160\n",
      "i=192\n",
      "i=224\n",
      "i=256\n",
      "i=288\n",
      "i=320\n",
      "i=352\n",
      "i=384\n",
      "i=416\n",
      "i=448\n",
      "i=480\n",
      "i=512\n",
      "i=544\n",
      "i=576\n",
      "i=608\n",
      "i=640\n",
      "i=672\n",
      "i=704\n",
      "i=736\n",
      "i=768\n",
      "i=800\n",
      "i=832\n",
      "i=864\n",
      "i=896\n",
      "i=928\n",
      "i=960\n",
      "i=992\n",
      "i=1024\n",
      "i=1056\n",
      "i=1088\n",
      "i=1120\n",
      "i=1152\n",
      "i=1184\n",
      "i=1216\n",
      "i=1248\n",
      "i=1280\n",
      "i=1312\n",
      "i=1344\n",
      "i=1376\n",
      "i=1408\n",
      "i=1440\n",
      "i=1472\n",
      "i=1504\n",
      "i=1536\n",
      "i=1568\n",
      "i=1600\n",
      "i=1632\n",
      "i=1664\n",
      "i=1696\n",
      "i=1728\n",
      "i=1760\n",
      "i=1792\n",
      "i=1824\n",
      "i=1856\n",
      "i=1888\n",
      "i=1920\n",
      "i=1952\n",
      "i=1984\n",
      "i=2016\n",
      "i=2048\n",
      "i=2080\n",
      "i=2112\n",
      "i=2144\n",
      "i=2176\n",
      "i=2208\n",
      "i=2240\n",
      "i=2272\n",
      "i=2304\n",
      "i=2336\n",
      "i=2368\n",
      "i=2400\n",
      "i=2432\n",
      "i=2464\n",
      "i=2496\n",
      "i=2528\n",
      "i=2560\n",
      "i=2592\n",
      "i=2624\n",
      "i=2656\n",
      "i=2688\n",
      "i=2720\n",
      "i=2752\n",
      "i=2784\n",
      "i=2816\n",
      "i=2848\n",
      "i=2880\n",
      "i=2912\n",
      "i=2944\n",
      "i=2976\n",
      "i=3008\n",
      "i=3040\n",
      "i=3072\n",
      "i=3104\n",
      "i=3136\n",
      "i=3168\n",
      "i=3200\n",
      "i=3232\n",
      "i=3264\n",
      "i=3296\n",
      "i=3328\n",
      "i=3360\n",
      "i=3392\n",
      "i=3424\n",
      "i=3456\n",
      "i=3488\n",
      "i=3520\n",
      "i=3552\n",
      "i=3584\n",
      "i=3616\n",
      "i=3648\n",
      "i=3680\n",
      "i=3712\n",
      "i=3744\n",
      "i=3776\n",
      "i=3808\n",
      "i=3840\n",
      "i=3872\n",
      "i=3904\n",
      "i=3936\n",
      "i=3968\n",
      "i=4000\n",
      "i=4032\n",
      "i=4064\n",
      "i=4096\n",
      "i=4128\n",
      "i=4160\n",
      "i=4192\n",
      "i=4224\n",
      "i=4256\n",
      "i=4288\n",
      "i=4320\n",
      "i=4352\n",
      "i=4384\n",
      "i=4416\n",
      "i=4448\n",
      "i=4480\n",
      "i=4512\n",
      "i=4544\n",
      "i=4576\n",
      "i=4608\n",
      "i=4640\n",
      "i=4672\n",
      "i=4704\n",
      "i=4736\n",
      "i=4768\n",
      "i=4800\n",
      "i=4832\n",
      "i=4864\n",
      "i=4896\n",
      "i=4928\n",
      "i=4960\n",
      "i=4992\n",
      "i=5024\n",
      "i=5056\n",
      "i=5088\n",
      "i=5120\n",
      "i=5152\n",
      "i=5184\n",
      "i=5216\n",
      "i=5248\n",
      "i=5280\n",
      "i=5312\n",
      "i=5344\n",
      "i=5376\n",
      "i=5408\n",
      "i=5440\n",
      "i=5472\n",
      "i=5504\n",
      "i=5536\n",
      "i=5568\n",
      "i=5600\n",
      "i=5632\n",
      "i=5664\n",
      "i=5696\n",
      "i=5728\n",
      "i=5760\n",
      "i=5792\n",
      "i=5824\n",
      "i=5856\n",
      "i=5888\n",
      "i=5920\n",
      "i=5952\n",
      "i=5984\n",
      "i=6016\n",
      "i=6048\n",
      "i=6080\n",
      "i=6112\n",
      "i=6144\n",
      "i=6176\n",
      "i=6208\n",
      "i=6240\n",
      "i=6272\n",
      "i=6304\n",
      "i=6336\n",
      "i=6368\n",
      "i=6400\n",
      "i=6432\n",
      "i=6464\n",
      "i=6496\n",
      "i=6528\n",
      "i=6560\n",
      "i=6592\n",
      "i=6624\n",
      "i=6656\n",
      "i=6688\n",
      "i=6720\n",
      "i=6752\n",
      "i=6784\n",
      "i=6816\n",
      "i=6848\n",
      "i=6880\n",
      "i=6912\n",
      "i=6944\n",
      "i=6976\n",
      "i=7008\n",
      "i=7040\n",
      "i=7072\n",
      "i=7104\n",
      "i=7136\n",
      "i=7168\n",
      "i=7200\n",
      "i=7232\n",
      "i=7264\n",
      "i=7296\n",
      "i=7328\n",
      "i=7360\n",
      "i=7392\n",
      "i=7424\n",
      "i=7456\n",
      "i=7488\n",
      "i=7520\n",
      "i=7552\n",
      "i=7584\n",
      "i=7616\n",
      "i=7648\n",
      "i=7680\n",
      "i=7712\n",
      "i=7744\n",
      "i=7776\n",
      "i=7808\n",
      "i=7840\n",
      "i=7872\n",
      "i=7904\n",
      "i=7936\n",
      "i=7968\n",
      "i=8000\n",
      "i=8032\n",
      "i=8064\n",
      "i=8096\n",
      "i=8128\n",
      "i=8160\n",
      "i=8192\n",
      "i=8224\n",
      "i=8256\n",
      "i=8288\n",
      "i=8320\n",
      "i=8352\n",
      "i=8384\n",
      "i=8416\n",
      "i=8448\n",
      "i=8480\n",
      "i=8512\n",
      "i=8544\n",
      "i=8576\n",
      "i=8608\n",
      "i=8640\n",
      "i=8672\n",
      "i=8704\n",
      "i=8736\n",
      "i=8768\n",
      "i=8800\n",
      "i=8832\n",
      "i=8864\n",
      "i=8896\n",
      "i=8928\n",
      "i=8960\n",
      "i=8992\n",
      "i=9024\n",
      "i=9056\n",
      "i=9088\n",
      "i=9120\n",
      "i=9152\n",
      "i=9184\n",
      "i=9216\n",
      "i=9248\n",
      "i=9280\n",
      "i=9312\n",
      "i=9344\n",
      "i=9376\n",
      "i=9408\n",
      "i=9440\n",
      "i=9472\n",
      "i=9504\n",
      "i=9536\n",
      "i=9568\n",
      "i=9600\n",
      "i=9632\n",
      "i=9664\n",
      "i=9696\n",
      "i=9728\n",
      "i=9760\n",
      "i=9792\n",
      "i=9824\n",
      "i=9856\n",
      "i=9888\n",
      "i=9920\n",
      "i=9952\n",
      "i=9984\n",
      "j=0\n",
      "j=32\n",
      "j=64\n",
      "j=96\n",
      "j=128\n",
      "j=160\n",
      "j=192\n",
      "j=224\n",
      "j=256\n",
      "j=288\n",
      "j=320\n",
      "j=352\n",
      "j=384\n",
      "j=416\n",
      "j=448\n",
      "j=480\n",
      "j=512\n",
      "j=544\n",
      "j=576\n",
      "j=608\n",
      "j=640\n",
      "j=672\n",
      "j=704\n",
      "j=736\n",
      "j=768\n",
      "j=800\n",
      "j=832\n",
      "j=864\n",
      "j=896\n",
      "j=928\n",
      "j=960\n",
      "j=992\n",
      "j=1024\n",
      "j=1056\n",
      "j=1088\n",
      "j=1120\n",
      "j=1152\n",
      "j=1184\n",
      "j=1216\n",
      "j=1248\n",
      "j=1280\n",
      "j=1312\n",
      "j=1344\n",
      "j=1376\n",
      "j=1408\n",
      "j=1440\n",
      "j=1472\n",
      "j=1504\n",
      "j=1536\n",
      "j=1568\n",
      "j=1600\n",
      "j=1632\n",
      "j=1664\n",
      "j=1696\n",
      "j=1728\n",
      "j=1760\n",
      "j=1792\n",
      "j=1824\n",
      "j=1856\n",
      "j=1888\n",
      "j=1920\n",
      "j=1952\n",
      "j=1984\n",
      "j=2016\n",
      "j=2048\n",
      "j=2080\n",
      "j=2112\n",
      "j=2144\n",
      "j=2176\n",
      "j=2208\n",
      "j=2240\n",
      "j=2272\n",
      "j=2304\n",
      "j=2336\n",
      "j=2368\n",
      "j=2400\n",
      "j=2432\n",
      "j=2464\n",
      "j=2496\n",
      "j=2528\n",
      "j=2560\n",
      "j=2592\n",
      "j=2624\n",
      "j=2656\n",
      "j=2688\n",
      "j=2720\n",
      "j=2752\n",
      "j=2784\n",
      "j=2816\n",
      "j=2848\n",
      "j=2880\n",
      "j=2912\n",
      "j=2944\n",
      "j=2976\n",
      "j=3008\n",
      "j=3040\n",
      "j=3072\n",
      "j=3104\n",
      "j=3136\n",
      "j=3168\n",
      "j=3200\n",
      "j=3232\n",
      "j=3264\n",
      "j=3296\n",
      "j=3328\n",
      "j=3360\n",
      "j=3392\n",
      "j=3424\n",
      "j=3456\n",
      "j=3488\n",
      "j=3520\n",
      "j=3552\n",
      "j=3584\n",
      "j=3616\n",
      "j=3648\n",
      "j=3680\n",
      "j=3712\n",
      "j=3744\n",
      "j=3776\n",
      "j=3808\n",
      "j=3840\n",
      "j=3872\n",
      "j=3904\n",
      "j=3936\n",
      "j=3968\n",
      "j=4000\n",
      "j=4032\n",
      "j=4064\n",
      "j=4096\n",
      "j=4128\n",
      "j=4160\n",
      "j=4192\n",
      "j=4224\n",
      "j=4256\n",
      "j=4288\n",
      "j=4320\n",
      "j=4352\n",
      "j=4384\n",
      "j=4416\n",
      "j=4448\n",
      "j=4480\n",
      "j=4512\n",
      "j=4544\n",
      "j=4576\n",
      "j=4608\n",
      "j=4640\n",
      "j=4672\n",
      "j=4704\n",
      "j=4736\n",
      "j=4768\n",
      "j=4800\n",
      "j=4832\n",
      "j=4864\n",
      "j=4896\n",
      "j=4928\n",
      "j=4960\n",
      "j=4992\n",
      "j=5024\n",
      "j=5056\n",
      "j=5088\n",
      "j=5120\n",
      "j=5152\n",
      "j=5184\n",
      "j=5216\n",
      "j=5248\n",
      "j=5280\n",
      "j=5312\n",
      "j=5344\n",
      "j=5376\n",
      "j=5408\n",
      "j=5440\n",
      "j=5472\n",
      "j=5504\n",
      "j=5536\n",
      "j=5568\n",
      "j=5600\n",
      "j=5632\n",
      "j=5664\n",
      "j=5696\n",
      "j=5728\n",
      "j=5760\n",
      "j=5792\n",
      "j=5824\n",
      "j=5856\n",
      "j=5888\n",
      "j=5920\n",
      "j=5952\n",
      "j=5984\n",
      "j=6016\n",
      "j=6048\n",
      "j=6080\n",
      "j=6112\n",
      "j=6144\n",
      "j=6176\n",
      "j=6208\n",
      "j=6240\n",
      "j=6272\n",
      "j=6304\n",
      "j=6336\n",
      "j=6368\n",
      "j=6400\n",
      "j=6432\n",
      "j=6464\n",
      "j=6496\n",
      "j=6528\n",
      "j=6560\n",
      "j=6592\n",
      "j=6624\n",
      "j=6656\n",
      "j=6688\n",
      "j=6720\n",
      "j=6752\n",
      "j=6784\n",
      "j=6816\n",
      "j=6848\n",
      "j=6880\n",
      "j=6912\n",
      "j=6944\n",
      "j=6976\n",
      "j=7008\n",
      "j=7040\n",
      "j=7072\n",
      "j=7104\n",
      "j=7136\n",
      "j=7168\n",
      "j=7200\n",
      "j=7232\n",
      "j=7264\n",
      "j=7296\n",
      "j=7328\n",
      "j=7360\n",
      "j=7392\n",
      "j=7424\n",
      "j=7456\n",
      "j=7488\n",
      "j=7520\n",
      "j=7552\n",
      "j=7584\n",
      "j=7616\n",
      "j=7648\n",
      "j=7680\n",
      "j=7712\n",
      "j=7744\n",
      "j=7776\n",
      "j=7808\n",
      "j=7840\n",
      "j=7872\n",
      "j=7904\n",
      "j=7936\n",
      "j=7968\n",
      "j=8000\n",
      "j=8032\n",
      "j=8064\n",
      "j=8096\n",
      "j=8128\n",
      "j=8160\n",
      "j=8192\n",
      "j=8224\n",
      "j=8256\n",
      "j=8288\n",
      "j=8320\n",
      "j=8352\n",
      "j=8384\n",
      "j=8416\n",
      "j=8448\n",
      "j=8480\n",
      "j=8512\n",
      "j=8544\n",
      "j=8576\n",
      "j=8608\n",
      "j=8640\n",
      "j=8672\n",
      "j=8704\n",
      "j=8736\n",
      "j=8768\n",
      "j=8800\n",
      "j=8832\n",
      "j=8864\n",
      "j=8896\n",
      "j=8928\n",
      "j=8960\n",
      "j=8992\n",
      "j=9024\n",
      "j=9056\n",
      "j=9088\n",
      "j=9120\n",
      "j=9152\n",
      "j=9184\n",
      "j=9216\n",
      "j=9248\n",
      "j=9280\n",
      "j=9312\n",
      "j=9344\n",
      "j=9376\n",
      "j=9408\n",
      "j=9440\n",
      "j=9472\n",
      "j=9504\n",
      "j=9536\n",
      "j=9568\n",
      "j=9600\n",
      "j=9632\n",
      "j=9664\n",
      "j=9696\n",
      "j=9728\n",
      "j=9760\n",
      "j=9792\n",
      "j=9824\n",
      "j=9856\n",
      "j=9888\n",
      "j=9920\n",
      "j=9952\n",
      "j=9984\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, val_data, val_label = utils.load_mnist_trainval()\n",
    "data, label = utils.load_mnist_test()\n",
    "batched_data, batched_label = utils.generate_batched_data(data, label, batch_size=32, shuffle=False, seed=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c68d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-1.48839468, -0.31530738],\n",
    "                      [-0.28271176, -1.00780433],\n",
    "                      [0.66435418, 1.2537461],\n",
    "                      [-1.64829182, 0.90223236]])\n",
    "y = np.array([[0.23629739, 0.76370261],\n",
    "              [0.67372745, 0.32627255],\n",
    "              [0.35677439, 0.64322561],\n",
    "              [0.07239128, 0.92760872]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd897383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23629739, 0.76370261],\n",
       "       [0.67372745, 0.32627255],\n",
       "       [0.35677439, 0.64322561],\n",
       "       [0.07239128, 0.92760872]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772e5730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.71937262e-09,  3.71937248e-09],\n",
       "       [ 1.59436375e-09, -1.59436364e-09],\n",
       "       [ 1.74628878e-09, -1.74628878e-09],\n",
       "       [ 1.62559298e-09, -1.62559288e-09]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83b0fa",
   "metadata": {},
   "source": [
    "## SIG AND DERIVSIG TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "923ed4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[-1.48839468, -0.31530738],\n",
    "              [-0.28271176, -1.00780433],\n",
    "              [0.66435418, 1.2537461],\n",
    "              [-1.64829182, 0.90223236]])\n",
    "B = np.array([[0.1841628, 0.4218198],\n",
    "              [0.42978908, 0.26740977],\n",
    "              [0.66023782, 0.77794766],\n",
    "              [0.16133995, 0.71140804]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb47f3b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sig = 1 / (1+np.exp(-A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54a1d8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0660873317757122e-08"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.sum(np.abs((sig - B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c402eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[-1.48839468, -0.31530738],\n",
    "              [-0.28271176, -1.00780433],\n",
    "              [0.66435418, 1.2537461],\n",
    "              [-1.64829182, 0.90223236]])\n",
    "D = np.array([[0.15024686, 0.24388786],\n",
    "              [0.24507043, 0.19590178],\n",
    "              [0.22432384, 0.1727451],\n",
    "              [0.13530937, 0.20530664]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55678e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15024686, 0.24388786],\n",
       "       [0.24507043, 0.19590178],\n",
       "       [0.22432384, 0.1727451 ],\n",
       "       [0.13530937, 0.20530664]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigd = 1 / (1+np.exp(-C))\n",
    "outD = sigd * (1-sigd)\n",
    "outD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1121b954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.28886890e-09, 9.90633417e-09])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.abs(D-outD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5876f8",
   "metadata": {},
   "source": [
    "## TEST RELU and DERIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "734408cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array([[-1.48839468, -0.31530738],\n",
    "              [-0.28271176, -1.00780433],\n",
    "              [0.66435418, 1.2537461],\n",
    "              [-1.64829182, 0.90223236]])\n",
    "F = np.array([[0.0, 0.0],\n",
    "              [0.0, 0.0],\n",
    "              [0.66435418, 1.2537461],\n",
    "              [0.0, 0.90223236]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a346c94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.abs(F - np.maximum(E, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "634a9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.array([[-1.48839468, -0.31530738],\n",
    "              [-0.28271176, -1.00780433],\n",
    "              [0.66435418, 1.2537461],\n",
    "              [-1.64829182, 0.90223236]])\n",
    "H = np.array([[0.0, 0.0],\n",
    "              [0.0, 0.0],\n",
    "              [1., 1.],\n",
    "              [0.0, 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "171fbd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(G>0).astype(float) == H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12e354",
   "metadata": {},
   "source": [
    "## TEST CROSS ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b30c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.2, 0.5, 0.3], [0.5, 0.1, 0.4], [0.3, 0.3, 0.4]])\n",
    "y = np.array([1, 2, 0])\n",
    "expected_loss = 0.937803\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8a8265f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.722533453766943e-07"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss = np.log(x)\n",
    "y_values = np.arange(len(y)),y\n",
    "expected_loss -np.mean(-log_loss[y_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c1a938e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69314718, -0.91629073, -1.2039728 ])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss[y_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e115c",
   "metadata": {},
   "source": [
    "## TEST ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd40abd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.4, 0.3])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.2, 0.5, 0.3], [0.5, 0.1, 0.4], [0.3, 0.3, 0.4]])\n",
    "y = np.array([1, 2, 0])\n",
    "y_values = x[np.arange(len(y)),y]\n",
    "expected_acc = 0.3333\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5b4b382b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.4, 0.3])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#acc = sum(x.max(axis=1) == x[y_values]) / y.size\n",
    "x[y_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, val_data, val_label = utils.load_mnist_trainval()\n",
    "data, label = utils.load_mnist_test()\n",
    "batched_data, batched_label = utils.generate_batched_data(data, label, batch_size=32, shuffle=False, seed=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeac41a",
   "metadata": {},
   "source": [
    "## softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1ee5c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, input_size=28 * 28, num_classes=10):\n",
    "    \"\"\"\n",
    "    A single layer softmax regression. The network is composed by:\n",
    "    a linear layer without bias => (activation) => Softmax\n",
    "    :param input_size: the input dimension\n",
    "    :param num_classes: the number of classes in total\n",
    "    \"\"\"\n",
    "    super().__init__(input_size, num_classes)\n",
    "    self._weight_init()\n",
    "\n",
    "def _weight_init(self):\n",
    "    '''\n",
    "    initialize weights of the single layer regression network. No bias term included.\n",
    "    :return: None; self.weights is filled based on method\n",
    "    - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n",
    "    '''\n",
    "    np.random.seed(1024)\n",
    "    self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n",
    "    self.gradients['W1'] = np.zeros((self.input_size, self.num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3b5dabd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'softmax_regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoftmax_regression\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'softmax_regression'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
